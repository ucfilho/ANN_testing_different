{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_teste_003_out_05_2019.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/ANN_testing_different/blob/master/ANN_teste_003_out_05_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdvajUEyxoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import _pickle as cPickle\n",
        "import gzip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSaVvomJe5oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    #Patch='https://github.com/mnielsen/neural-networks-and-deep-learning/data/mnist.pkl.gz'\n",
        "    #Patch='https://github.com/mnielsen/rmnist/blob/master/data/mnist.pkl.gz'\n",
        "    #Patch='https://www.kaggle.com/pablotab/mnistpkl.gz'\n",
        "    #deeplearning.net/data/mnist/mnistpkl.gz\n",
        "    Patch='mnist.pkl.gz'\n",
        "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
        "    f = gzip.open(Patch, 'rb')\n",
        "    training_data, validation_data, test_data = cPickle.load(f, encoding='bytes')\n",
        "    f.close()\n",
        "    return training_data, validation_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpVdJG3me6KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://52.32.3.36/post/6\n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2s6XlYGe6Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(self, X, y, X_val, y_val, nb_epoch, batch_size, eta):\n",
        "    n = len(X)\n",
        "    for i in range(nb_epoch):\n",
        "        epoch_loss = 0\n",
        "        X, y = shuffle(X, y)\n",
        "        for j in range(0, n, batch_size):\n",
        "            X_batch = X[j:j + batch_size]\n",
        "            y_batch = y[j:j + batch_size]\n",
        "            loss, grads = self.loss(X_batch, y_batch)\n",
        "            epoch_loss += loss\n",
        "            # update parameters\n",
        "            for param_name in ('W1', 'b1', 'W2', 'b2'):\n",
        "                self.params[param_name] -= eta * grads[param_name]\n",
        "        train_acc = self.evaluate(X, y)\n",
        "        val_acc = self.evaluate(X_val, y_val)\n",
        "        print(\"epoch %d / %d: loss %f, train_acc: %f, val_acc: %f\" %\n",
        "              (i + 1, nb_epoch, epoch_loss / n, train_acc, val_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaSpUi8rnGnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReLU(x):\n",
        "    return np.maximum(0, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52qkipw2o7NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout(x, dropout_p):\n",
        "    return x * np.random.binomial([np.ones(x.shape)], 1 - dropout_p)[0] / (1 - dropout_p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGi0t9gTm811",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IUIaV8lyo84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    \"\"\"\n",
        "    Architecture:\n",
        "        Flatten -> Dense -> ReLU -> Dropout -> Dense -> SoftMax\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_p):\n",
        "        self.params = {\n",
        "            'W1': np.random.randn(input_size, hidden_size) / np.sqrt(input_size),\n",
        "            'b1': np.zeros((1, hidden_size)),\n",
        "            'W2': np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size),\n",
        "            'b2': np.zeros((1, output_size))\n",
        "        }\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, nb_epoch, batch_size, eta):\n",
        "        n = len(X)\n",
        "        for i in range(nb_epoch):\n",
        "            epoch_loss = 0\n",
        "            X, y = shuffle(X, y)\n",
        "            for j in range(0, n, batch_size):\n",
        "                X_batch = X[j:j + batch_size]\n",
        "                y_batch = y[j:j + batch_size]\n",
        "                loss, grads = self.loss(X_batch, y_batch)\n",
        "                epoch_loss += loss\n",
        "                # update parameters\n",
        "                for param_name in ('W1', 'b1', 'W2', 'b2'):\n",
        "                    self.params[param_name] -= eta * grads[param_name]\n",
        "            train_acc = self.evaluate(X, y)\n",
        "            val_acc = self.evaluate(X_val, y_val)\n",
        "            print(\"epoch %d / %d: loss %f, train_acc: %f, val_acc: %f\" %\n",
        "                  (i + 1, nb_epoch, epoch_loss / n, train_acc, val_acc))\n",
        "    def loss(self, X, y):\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "\n",
        "        n = X.shape[0]\n",
        "\n",
        "        # feed forward pass\n",
        "        h1 = ReLU(np.dot(X, W1) + b1)\n",
        "        h1 = dropout(h1, dropout_p=self.dropout_p)\n",
        "        out = np.dot(h1, W2) + b2\n",
        "        probs = softmax(out)\n",
        "\n",
        "        # loss\n",
        "        log_probs = -np.log(probs[range(n), y])\n",
        "        loss = np.sum(log_probs) / n\n",
        "\n",
        "        # backward pass\n",
        "        dout = probs\n",
        "        dout[range(n), y] -= 1\n",
        "        dh1 = np.dot(dout, W2.T)\n",
        "        dh1[h1 <= 0] = 0\n",
        "        dW2 = np.dot(h1.T, dout)\n",
        "        db2 = np.sum(dout, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, dh1)\n",
        "        db1 = np.sum(dh1, axis=0, keepdims=True)\n",
        "\n",
        "        grads = {\n",
        "            'W1': dW1,\n",
        "            'b1': db1,\n",
        "            'W2': dW2,\n",
        "            'b2': db2\n",
        "        }\n",
        "        return loss, grads\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        h1 = ReLU(np.dot(X, self.params['W1']) + self.params['b1'])\n",
        "        out = np.dot(h1, self.params['W2']) + self.params['b2']\n",
        "        probs = softmax(out)\n",
        "        pred = np.argmax(probs, axis=1)\n",
        "        return sum(pred == y) / X.shape[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIxbS5uApmSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "e7501b28-5308-4d06-c62e-d8630daadca2"
      },
      "source": [
        "training_data, validation_data, test_data = load_data()\n",
        "X, y = training_data\n",
        "X_val, y_val = validation_data\n",
        "X_test, y_test = test_data\n",
        "model = Model(input_size=784, hidden_size=512, output_size=10, dropout_p=0.2)\n",
        "model.train(X, y, X_val, y_val, nb_epoch=5, eta=0.01, batch_size=10)\n",
        "print(\"Test set accuracy\", model.evaluate(X_test, y_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-06 02:04:55--  http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
            "Resolving deeplearning.net (deeplearning.net)... 132.204.26.28\n",
            "Connecting to deeplearning.net (deeplearning.net)|132.204.26.28|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16168813 (15M) [application/x-gzip]\n",
            "Saving to: ‘mnist.pkl.gz.4’\n",
            "\n",
            "mnist.pkl.gz.4      100%[===================>]  15.42M  4.39MB/s    in 4.0s    \n",
            "\n",
            "2019-10-06 02:05:00 (3.86 MB/s) - ‘mnist.pkl.gz.4’ saved [16168813/16168813]\n",
            "\n",
            "epoch 1 / 5: loss 0.025252, train_acc: 0.967020, val_acc: 0.965500\n",
            "epoch 2 / 5: loss 0.011288, train_acc: 0.978480, val_acc: 0.974200\n",
            "epoch 3 / 5: loss 0.007959, train_acc: 0.979860, val_acc: 0.971800\n",
            "epoch 4 / 5: loss 0.006292, train_acc: 0.988380, val_acc: 0.977800\n",
            "epoch 5 / 5: loss 0.004992, train_acc: 0.991520, val_acc: 0.977300\n",
            "Test set accuracy 0.9801\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}