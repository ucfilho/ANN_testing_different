{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_teste_004_out_08_2019.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/ANN_testing_different/blob/master/ANN_teste_004_out_08_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b51QjyH0uKAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://ai-news.ru/2017/10/kratkij_kurs_mashinnogo_obucheniya_ili_kak_sozdat_nejronnuu_set_dly.html\n",
        "# https://weekly-geekly.github.io/articles/340792/index.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBlVIGyOrUBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random  \n",
        "def generate_population(p, w_size):     \n",
        "  population = []     \n",
        "  for i in range(p):         \n",
        "    model = []         \n",
        "    for j in range(w_size + 1): # +1 for b (bias term)             \n",
        "    model.append(2 * random.random() - 1)  \n",
        "    # random initialization from -1 to 1 for b and w         \n",
        "    population.append(model)     \n",
        "  return np.array(population)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUKSGqeosQTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(X, Y, model):     \n",
        "  A = 0     \n",
        "  m = len(Y)     \n",
        "  for i, y in enumerate(Y):         \n",
        "    A += (1/m)*(y*(1 if neuron(X[i], model) >= 0.5 else 0)+(1-y)*(0 if neuron(X[i], model) >= 0.5 else 1))     \n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56SyW-Osqyyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_crossentropy(X, Y, model): \n",
        "  # loss function     \n",
        "  J = 0     \n",
        "  m = len(Y)     \n",
        "  for i, y in enumerate(Y):         \n",
        "    J += -(1/m)*(y*np.log(neuron(X[i], model))+(1.-y)*np.log(1.-neuron(X[i], model)))     \n",
        "  return J"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lPmp3ltrCpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evolution(population, X_in, Y, number_of_generations, children):     \n",
        "  for i in range(number_of_generations):         \n",
        "    X = [[1]+[v.tolist()] for v in X_in]         \n",
        "    offspring = []         \n",
        "    for genom in population:             \n",
        "      for j in range(children):                 \n",
        "        child = mutation(genom)                 \n",
        "        child_loss = 1 - accuracy(X_in, Y, child) \n",
        "        # or child_loss = binary_crossentropy(X, Y, child)  is better                 \n",
        "        offspring.append([child_loss, child])             \n",
        "        population = selection(offspring, population)     \n",
        "  return population"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRTUNS4urm0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "def neuron(x, w):     \n",
        "  z = np.dot(w, x)     \n",
        "  output = activation(z)     \n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwe_AA4Orzf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(X):     \n",
        "  return (X-X.mean())/X.std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhDqAOMfr_8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(z):     \n",
        "  return 1/(1+np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51sBv1iYofo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(model, X_in, Y, number_of_iteratons=500, learning_rate=0.1):\n",
        "  X = [[1]+[v.tolist()] for v in X_in]\n",
        "  m = len(Y)\n",
        "  for it in range(number_of_iteratons):\n",
        "    new_model = []\n",
        "    for j, w in enumerate(model):\n",
        "      error = 0\n",
        "      for i, x in enumerate(X):\n",
        "        error += (1/m) * (neuron(X[i], model) — Y[i]) * X[i][j]\n",
        "        w_new = w — learning_rate * error\n",
        "        new_model.append(w_new)\n",
        "        model = new_model\n",
        "        model_loss = binary_crossentropy(X, Y, model)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}